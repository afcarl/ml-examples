{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char RNN\n",
    "short introduction to char-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lfs=require 'lfs'\n",
    "lfs.chdir(\"/root/shared/ml-examples/karpathy/char-rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "At first, let's generate text like Shakespear style from pretrained model.\n",
    "\n",
    "You can change following parameters, try various.\n",
    "\n",
    "```\n",
    "Options\n",
    "  <model>      model checkpoint to use for sampling\n",
    "  -seed        random number generator's seed [123]\n",
    "  -sample       0 to use max at each timestep, 1 to sample at each timestep [1]\n",
    "  -primetext   used as a prompt to \"seed\" the state of the LSTM using a given sequence, before we sample. []\n",
    "  -length      number of characters to sample [2000]\n",
    "  -temperature temperature of sampling [1]\n",
    "  -gpuid       which gpu to use. -1 = use CPU [0]\n",
    "  -opencl      use OpenCL (instead of CUDA) [0]\n",
    "  -verbose     set to 0 to ONLY print the sampled text, no diagnostics [1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = \"cv/lm_lstm_epoch4.73_1.5523.t7\"\n",
    "seed = 123\n",
    "sample = 1\n",
    "prime_text = \"the meaning of life is \"\n",
    "length = 500\n",
    "temperature = 1\n",
    "\n",
    "os.execute(\n",
    "    \"th sample.lua \"..model..\n",
    "    \" -seed \"..seed..\n",
    "    \" -sample \"..sample..\n",
    "    \" -primetext \\\"\"..prime_text..\"\\\"\"..\n",
    "    \" -length \"..length..\n",
    "    \" -temperature \"..temperature..\n",
    "    \" -gpuid -1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CharRNN outputs the probability what character comes next at each iteration.\n",
    "When `-sample` is set to 1, it samples from probability distribution which CharRNN outputs.\n",
    "\n",
    "Otherwise, if `-sample` is set to 0, it tends to generate such repeated text as you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "creating an lstm...\t\n",
       "seeding with the meaning of life is \t\n",
       "--------------------------\t\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "the meaning of life is the such and stand\n",
       "That shall be the words and so much and the such and stands\n",
       "That shall stay the such a supply to the such and stands\n",
       "That shall stay the such a supply to the such and stands\n",
       "That shall stay the such a supply to the such and stands\n",
       "That shall stay the such a supply to the such and stands\n",
       "That shall stay the such a supply to the such and stands\n",
       "That shall stay the such a supply to\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "\n",
    "os.execute(\n",
    "    \"th sample.lua \"..model..\n",
    "    \" -seed \"..seed..\n",
    "    \" -sample \"..sample..\n",
    "    \" -primetext \\\"\"..prime_text..\"\\\"\"..\n",
    "    \" -length \"..length..\n",
    "    \" -temperature \"..temperature..\n",
    "    \" -gpuid -1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "You can also train your own data.\n",
    "If you would like to use your own data, then create a single file input.txt and place it into a folder such as `somefolder/input.txt` for example. \n",
    "\n",
    "Then you should specify: \n",
    "\n",
    "`-data_dir somefolder`\n",
    "\n",
    "\n",
    "Following parameters are changable, minimum parameters you should specify are data_dir and checkpoint_dir.\n",
    "\n",
    "```\n",
    "Options\n",
    "  -data_dir                  data directory. Should contain the file input.txt with input data [data/tinyshakespeare]\n",
    "  -rnn_size                  size of LSTM internal state [128]\n",
    "  -num_layers                number of layers in the LSTM [2]\n",
    "  -model                     lstm,gru or rnn [lstm]\n",
    "  -learning_rate             learning rate [0.002]\n",
    "  -learning_rate_decay       learning rate decay [0.97]\n",
    "  -learning_rate_decay_after in number of epochs, when to start decaying the learning rate [10]\n",
    "  -decay_rate                decay rate for rmsprop [0.95]\n",
    "  -dropout                   dropout for regularization, used after each RNN hidden layer. 0 = no dropout [0]\n",
    "  -seq_length                number of timesteps to unroll for [50]\n",
    "  -batch_size                number of sequences to train on in parallel [50]\n",
    "  -max_epochs                number of full passes through the training data [50]\n",
    "  -grad_clip                 clip gradients at this value [5]\n",
    "  -train_frac                fraction of data that goes into train set [0.95]\n",
    "  -val_frac                  fraction of data that goes into validation set [0.05]\n",
    "  -init_from                 initialize network parameters from checkpoint at this path []\n",
    "  -seed                      torch manual random number generator seed [123]\n",
    "  -print_every               how many steps/minibatches between printing out the loss [1]\n",
    "  -eval_val_every            every how many iterations should we evaluate on validation data? [1000]\n",
    "  -checkpoint_dir            output directory where checkpoints get written [cv]\n",
    "  -savefile                  filename to autosave the checkpont to. Will be inside checkpoint_dir/ [lstm]\n",
    "  -accurate_gpu_timing       set this flag to 1 to get precise timings when using GPU. Might make code bit slower but reports accurate timings. [0]\n",
    "  -gpuid                     which gpu to use. -1 = use CPU [0]\n",
    "  -opencl                    use OpenCL (instead of CUDA) [0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_root = \"/root/shared/ml-examples/workshop/char_rnn/\"\n",
    "data_dir = nb_root..\"testdata\"\n",
    "checkpoint_dir = nb_root..\"cv\"\n",
    "\n",
    "os.execute(\n",
    "    \"th train.lua \"..\n",
    "    \" -data_dir \\\"\"..data_dir..\"\\\"\"..\n",
    "    \" -checkpoint_dir \\\"\"..checkpoint_dir..\"\\\"\"..\n",
    "    \" -gpuid -1\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
